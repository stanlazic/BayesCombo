---
title: "BayesCombo"
author: "Bruno Contrino"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{BayesCombo}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

---
title: "Project"
output:
  html_document:
    toc: true 
    toc_depth: 3  
    number_section: true  
    theme: united
author: "Bruno Contrino"
date: "2 June 2016"
 
---

# Introduction

In the paper *'Combining Statistical Evidence From Several Studies: A Method Using Bayesian Updating and an Example From Research on Trust Problems in Social and Economical Exchange'* by Kuiper et al, the authors use Bayes factors to combine a series of Studies and quantify the joint evidence in multiple studies regarding the effects of one variable.

The method will use Bayes Factors which quanitfy evidence for several hypotheses. Th Bayes factor gives the relative support for each hypothesis allowing one to make statements such as the following  $H_{>}$ is $x$ times more likely that $H_{<}$. 

The input for Bayesian updating is the estimate of the parameter of interest and its standard error which can be otained from datasets or directly form published studies. Something that must be considered is that it is the person who implements the method which must decide whether the different studies or data sets which will be studied do indeed offer a comparable relationship between two key variables. 


# The method 

In this section I will give a detailed walkthrough of the method which will then be followed by a walkthrough of the code, which will be less detailed but may contain the most parts which I believe help clarify the code in as well. 

Evidence will be calculated for three hypotheses: $H_{0}$ , $H_{>}$,  $H_{<}$ where: 

$$
\begin{aligned}
H_{0} &= \textrm{No effect} \\
H_{>} &= \textrm{Positive effect} \\
H_{<} &= \textrm{Negative effect} \\
\end{aligned}
$$


## Step One: Define Prior Model Probabilities 

The prior model probability (PrMP), denoted as $\pi^{0}_{m} \,  \textrm{for} \, m\, \in  \{0,>,< \}$, is a number on a scale of 0 to 1, which quantifies the weight attached to the current hypothesis. Subsequently, we start with one study and use its parameter estimate and standard error to calculate or approximate the likelihood. Based on this, the Bayes factor for each hypothesis is determined.

To initialise, the PrMPs are given equal weight and thus: $\pi_{m}^{0} = \frac{1}{3}$. 

## Step Two: Computing Prior Variance

To evaluate the hypotheses $H_{m} \,  \textrm{for} \, m\, \in  \{0,>,< \}$ the Distribution of $H_{m}$ needs to be specified. Kuiper et al use a conjugate prior (Gelman et al. 2004), which implies that the prior distribution of the parameter $\beta_{1} is normal, which in turn will result in a normal posterior. To determine the prior distribution of $H_{m}$ one needs to consider the case in which the parameter $\beta_{1}$ for the unconstrained case. 

The unconstrained prior is defined by: $P(\beta_{1}) = \mathcal{N}(\beta_{0}, \sigma_{0}^{2})$

Becasue the belief in either the positive or negative effect should be the same we set $\beta_{0} = 0$. Therefore we now need to calculate the prior variance in order to have the parameters for the unconstrained prior. 

The literature (Kuiper et al and others) suggest that the prior variance be 'vague/non-informative such that is, such that it has little influence on the results. But, it should not be too vague, because then $H_{0}$ will receive the highest support also when it is not true. This is known as the Lindley paradox (Lindley 1957). Grounding the prior variance on the data avoids having too vague priors (Berger and Pericchi 2004, 1996).' 

The method used by Kuiper et al is propesed by Klugkist, Laudy, and Hoijtink (2005) and is as follows:

- Calculate the 99% confidence intervals of the observed mean and standard deviation. 

- Choose the bound which will maximise the difference from the prior mean (zero in our case).

- Use this to set the prior variance. 

## Step Three: Use Prior variance and mean to calculate the unconstrained posterior distribution parametrs

Once the prior variances have been calculated, they can be used along side the prior mean $\beta_{0}$ and the observed means and variances to calculate the unconstrained posterior means and variances. These are computed for $t = 1, ... ,T$ where $T = \textrm{Total number of Studies}$ , using the following: 

$$
\beta^{'}_{t} = \frac{\frac{\hat{\beta}_{t}}{\hat{\sigma}_{t}^{2}}+\frac{\beta_{0}}{\sigma^{2}_{0,t}}}{\frac{1}{\hat{\sigma}^{2}_{t}}+\frac{1}{\sigma^{2}_{0,t}}} \, , \, \sigma^{'2}_{t} = \frac{1}{\frac{1}{\hat{\sigma}^{2}_{t}} + \frac{1}{\sigma^{2}_{0,t}}}
$$


Where $\hat{\beta}_{t}$ is the observed mean and  $\hat{\sigma}^{2}_{t}$ is the observed variance for study $t$. Then the unconstrained posterior distribution will be $\mathcal{N}(\beta^{'}, \sigma^{'2})$. 


## Step Four: Calculating Bayes Factors for a constrained Hypothesis with respect to the unconstrained hypothesis. 

Once the unconstrained prior and posterior distribution parameters have been calculated, the Bayes Factors for a constrained Hypothesis with respect to the unconstrained hypothesis can be calculated. These will be denoted as $BF_{m,u} \,  \textrm{for} \, m\, \in  \{0,>,< \}$.

Kuiper et al describe these values as a 'useful technical tool' as the unconstrained hypothesis isn't considered. 

The reason it is useful is because the Bayes Factors with respect to other hypotheses can be calculated from these values via the following calculation: $H_{m,} \, , H_{m',} \,  \textrm{for} \, m \, , m^{'} \, \in  \{0,>,< \}$: 

$$
BF_{m,m^{'}} = \frac{BF_{m,u}}{BF_{m^{'},u}}
$$

To calculate $BF_{m,u}  \textrm{ for } m \in \{ >,<\}$:

$$
BF_{m,u} = \frac{f_{m}}{c_{m}}
$$

Where $c_{m} \textrm{ and } f_{m}$ are the proportions of the unconstrained prior in  $P(\beta_{1}) = \mathcal{N}(\beta_{0}, \sigma_{0}^{2})$ and unconstrained posterior in $P( \beta_{1} | y) \approx \mathcal{N}(\beta^{'},\sigma^{'})$ respectively, in agreement with constraints of hypothesis $H_{m}\textrm{ for } m \in \{ >,<\}$.

Now we chose $\beta_{0} = 0$ such that half of the prior distribution would be in agreement with $H_{>}$ and the other half would be in agreement with $H_{<}$. Therefore by definition $c_{m} = \frac{1}{2}$.

To calculate $BF_{0,u}$ I use a result by Dickey (1971):

$$
BF_{0,u} = \frac{P(\beta_{1} = 0 | y)}{P(\beta_{1} = 0)}
$$

So one can calculate $BF_{0,u}$ by evaluationg the unconstrained posterior and prior density at $\beta_{1} = 0$.


## Step Five: Calculating Posterior Model Probabilities

The Posterior Model Probability for a given hypothesis $H_{m}$ gives the relative support for $H_{m}$ for a finite set of hypotheses $m$. The PMP $\pi^{1}_{m}$ is calculated as follows:

$$
\pi^{1}_{m} = \frac{\pi^{0}_{m} BF_{mu}}{\pi^{0}_{0} BF_{0u} + \pi^{0}_{>} BF_{>u} + \pi^{0}_{<} BF_{<u}}
$$

Where $\pi^{0}_{m}$ is the Prior Model Probbility which, for the first step, is initialised as $\frac{1}{3}$ (see step 1).

## Step Six: Updating Posterior Model Probabilities

Now the evidence, for each hypothesis, from multiple studies must be combined. This is done by setting the PrMP of the study $t$ to the PMP of the study $t-1$. Then the updating step follows from this: 

$$
\pi^{0}_{t,m} = \pi^{1}_{t-1,m} \, \textrm{for }  t = 2, ... ,T
$$

Where $\pi^{1}_{t,m}$ is defined in the step above. 

This concludes the method.

Please see below for a full walkthrough of all the functions which have been written to carry out the above.

# The R Code

The various functions presented will be in order of when they are needed. For clarity on outputs and general flow I will use the real data example of Kuiper et al.

Below are the input parameter values: 

```{r examplesetup, echo = FALSE}

beta0<- 0
beta1<- c(0.090,0.140,1.090,1.781)
var1<- c(0.029,0.054,0.093,0.179)
var1<- var1^2
pi0<- rep(1/3,3)

```

## BFcombo class 

In order to create an S3 class for the method, an object of custom class 'BFcombo' is created and updated as the pipleine of the method continues. The object is made by the `make.BFcombo()` function which is called when running the prior.var.

```{r makebfcombo}

make.BFcombo<- function(beta1, var1, beta0, pi0){
  BFcombo<- structure(list(beta1 = beta1, var1 = var1,
                           beta0 = beta0, pi0 = pi0 ), class = "BFcombo")
  BFcombo

}

```


## Calculating prior variances: __prior.var__

This function computes the prior variance, The literature (Kuiper et al and others) suggest that the prior variance be 'vague/non-informative such that is, such that it has little influence on the results. But, it should not be too vague, because then $H_{0}$ will receive the highest support also when it is not true. This is known as the Lindley paradox (Lindley 1957). Grounding the prior variance on the data avoids having too vague priors (Berger and Pericchi 2004, 1996).' 

The method used by Kuiper et al is propesed by Klugkist, Laudy, and Hoijtink (2005) and is as follows:

- Calculate the 99% confidence intervals of the observed mean and standard deviation. 

- Choose the bound which will maximise the difference from the prior mean ( zero in out case).

- Use this to set the prior variance. 


```{r priorvar}

## variance of prior 
## REFERENCE : Combining Statistical evidence from several Studies , Kuiper et al, page 9 (69-70) and Figure 2.

prior.var<- function(beta1, var1, beta0, pi0, percent = 99){
  if(is.numeric(beta1) != TRUE){
    stop("beta1 is not numeric")
  }

  if(is.numeric(var1) != TRUE){
    stop("var1 is not numeric")
  }

  if(is.numeric(beta0) != TRUE){
    stop("beta0 is not numeric")
  }

  if(is.numeric(pi0) != TRUE){
    stop("pi0 is not numeric")
  } else if (sum(pi0) != 1){
    stop("Prior model probabilities do not sum to 1")
  }

  BFcombo<- make.BFcombo(beta1,var1,beta0,pi0)
    #setup
    percent<- (percent / 100)
    percent<- percent + (1-percent)/2
    multiplier<- qnorm(percent)
    data<- matrix(0, ncol = 2 , nrow = length(BFcombo$beta1))
    # get min/max
    data[,1]<- BFcombo$beta1 - sqrt(BFcombo$var1)*multiplier
    data[,2]<- BFcombo$beta1 + sqrt(BFcombo$var1)*multiplier

    bounds<- abs(data)
    pVar <- apply(X = bounds,MARGIN = 1,max)
    pVar <- ((pVar / multiplier))^2
    # update.object
    BFcombo$var0 <- pVar

    return(BFcombo)

}



```

### Summary of input and output

__Inputs__

- beta1	is a vector of observed means

- var1 is a vector of observed variances

- beta0	is a vector or value of prior means. This is usually 0. 

- pi0	is a vector of prior model probabilities.

- percent sets the confidence intervals used to calculate the prior variances. The default is a 99% C.I.

__Outputs__

The output consists of a BFcombo object with a new section containing a vector which contains the prior variances.
This vector has length of the number of studies and the prior variances are ordered in this vector such that position $i$ represents the prior variance of study $i$. 

**Example Output**

```{r expriorvar}

ex<- prior.var(beta1 = beta1, var1 = var1 , beta0 = beta0, pi0 = pi0 )
ex$var0 
```

After running this function we have the parameters of distribution of the unconstrained prior $P(\beta_{1})   \textrm{~} \mathcal{N}(\beta_{0} , \sigma_{0}^2)$

$$
\\
$$


##Calculating Unconstrained Posterior Parameters: __u.post.param__

The following function is used to calculate the posterior means and variations for the observed data. The following equations are used in order to do this: 

$$
\beta^{'} = \frac{\frac{\hat{\beta}}{\hat{\sigma}^{2}}+\frac{\beta_{0}}{\sigma^{2}_{0}}}{\frac{1}{\hat{\sigma}^{2}}+\frac{1}{\sigma^{2}_{0}}} , \sigma^{'} = \frac{1}{\frac{1}{\hat{\sigma}^{2}} + \frac{1}{\sigma^{2}_{0}}}
$$


```{r upostparam}

## Unconstrained posterior parameters 
## REFERENCE : Combining Statistical evidence from several Studies , Kuiper et al, page 9 (70) .


u.post.param<- function(BFcombo){
  if(any(names(BFcombo) == "var0")) {
    betaVar<- 1/ ((1/BFcombo$var0)  + (1/BFcombo$var1))
    betaTilda<-(BFcombo$beta0 * (1/BFcombo$var0)  + BFcombo$beta1 * (1/BFcombo$var1))*betaVar

    out<- rbind(betaTilda,betaVar)
    rownames(out)<- c("u.beta.post","u.var.post")
    BFcombo$unBetaPost<- out[1,]
    BFcombo$unVarPost<- out[2,]

    return(BFcombo)
  } else {
    if(class(BFcombo) != "BFcombo"){
      stop("Input not of BFcombo class please run prior.var function")
    }else{
      stop("Unconstrained prior variances not found. Please run prior.var function")
    }
  }

}

```


###Summary of input and output 

__Inputs__

prior.beta: This is the prior mean $\beta_{0}$ which I set to 0. 

prior.var: Takes the prior variances.This is the output produced from the prior.var function.

data.beta: This is the observed means of the studies. 

data.var: This is the observed variances of the studies. 

__Outputs__

The output of this function is a $2 \times T$ matrix where T is the number of studies. 
The first row consists of the unconstrained posterior model mean for each study $t \in T$.
The second row consists of the unconstrained posterior model variance for each study $t \in T$.

**Example Output**


```{r exupostvar}

ex<- u.post.param(ex)

ex$unBetaPost

ex$unVarPost
```

$$
\\
$$


##Getting Proportions of Unconstrained Prior: __sample.prop__

The following function gets the proportions of the unconstrained posterior ($f_{m}$) and the proportions of the unconstrained prior ($c_{m}$). These are required to calculate $BF_{mu}, m \in \{ >,<\}$ which is the bayes factor for a constrained hypothesis e.g $H_{>}$ with respect to the unconstrained hypothesis.

```{r sampldata}
## Unconstrained posterior proportions and Unconstrained prior proportions

## REFERENCE : Combining Statistical evidence from several Studies , Kuiper et al, page 72.
sample.prop<- function(n,prior.mean,posterior.mean,prior.sd,posterior.sd,hypothesis = 1){
  if(hypothesis == "1"){
    "%a%" = function(x,y){x > y}
  } else {
    "%a%" = function(x,y) {
      x < y
    }
  }

  n.post<- length(posterior.mean)
  Cm<- Fm<- names.vec<- rep(0,n.post)

  Cm[]<- 0.5

  for(i in 1:n.post){
    Fm[i]<- mean(ifelse(rnorm(n,posterior.mean[i],posterior.sd[i]) %a% prior.mean , 1 , 0))

    names.vec[i]<- paste("prob.post",i,sep = "")

  }

  out<- matrix(0,ncol = (n.post), nrow = 2)
  rownames(out)<- c("Fm","Cm")
  colnames(out)<- names.vec
  out[2,]<- Cm
  out[1,]<- Fm
  out
}

```

###Summary of input and output


__Inputs__


- n: Number of samples taken when running rnorm.

- prior.mean : Vctor of prior means.

- posterior.mean : vector of posterior means.

- prior.sd: Vector of prior standard deviations.

- posterior.sd: Vector of posterior standard deviations. 

- hypothesis = 1: This acts as a switch for the hypothesis sampled, can be changed to any other number and hypothesis 2 will be sampled instead. It isn't a crucial input and is more for flexibility and completeness. The analysis will work if this is never changed.

__Outputs__

The output of this function is a $2 \times T$ matrix where T is the number of studies. 
The first row consists of the unconstrained posterior model proportions for each study $t \in T$.
The second row consists of the unconstrained prior model proportions for each study $t \in T$.

**Example Output**

```{r exsmpldata, echo = FALSE}
sample.prop(n = 100000 ,prior.mean =  ex$beta0, posterior.mean = ex$unBetaPost, prior.sd = sqrt(ex$var0) , posterior.sd = sqrt(ex$unVarPost))

```

$$
\\
$$


##Calculating the unconstrained Bayes factors $H_{mu}$: __unconstrained.BF__

This function utilises the $c_{m}$ and $f_{m}$ produced in the previous function in order to calculate $BF_{mu} = \frac{f_{m}}{c_{m}}$ for $m \in \{ >, < \}$.

To $BF_{0u}$, the following formula is used in the function: $BF_{0u} = \frac{P(\beta_{1} = 0 |y )}{P(\beta_{1} = 0)}$ so one only has to compute the unconstrained prior and posterior at $beta_{1} = 0$. 


```{r bfmu}
## Calculating unconstrained Bayes Factors. 
## REFERENCE : Combining Statistical evidence from several Studies , Kuiper et al, page 72.

unconstrained.BF<- function(BFcombo,n = 1000){
  if(any(names(BFcombo) == "unBetaPost")){
    #BF_0u NULL
    n.studies<- length(BFcombo$unBetaPost)
    name.vec<- rep(NA,n.studies)
    for(i in 1:n.studies){
      name.vec[i]<- paste0("study",i)
    }
    BF.0u<- rep(0,n.studies)
    for( i in 1:n.studies){
      BF.0u[i]<- dnorm(BFcombo$beta0,BFcombo$unBetaPost[i],sqrt(BFcombo$unVarPost)[i])/dnorm(BFcombo$beta0,BFcombo$beta0,sqrt(BFcombo$var0)[i])
    }

    ## Run sampler for BF.1u
    temp.proportions<- sample.prop(n,BFcombo$beta0,BFcombo$unBetaPost,sqrt(BFcombo$var0),sqrt(BFcombo$unVarPost),hypothesis = 1)
    # BF_1u in the case m = >


    BF.1u<- temp.proportions[1,] /  temp.proportions[2,]

    # BF.2u in the case m = <

    BF.2u<- (1 - temp.proportions[1,]) / temp.proportions[2,]


    out<- rbind(BF.0u,BF.1u,BF.2u)
    rownames(out)<- c("H:0","H:>","H:<")
    colnames(out)<- name.vec
    BFcombo$BFmu<- out
    return(BFcombo)
  } else{
    if(class(BFcombo) == "BFcombo"){
      stop("Sections unBetaPost and unVarPost not present, please run u.post.params function")
    }else{
      stop("Object not of BFcombo class, please see make.BFcombo function")
    }
  }

}

```

##Summary of inputs and outputs

__Inputs__

- BFcombo is an object of class 'BFcombo'.
- n = 1000 : Number of random samples taken (used in sampl.data2).  

__Output__

The output of this function is an updated BFcombo object with a section containing a  $3 \times T$ matrix where T is the number of studies. 

The first row consists of the Bayes Factor $BF_{0u}$ for the hypothesis $H_{0}$ and the unconstrained model for each study $t \in T$.

The second row row consists of the Bayes Factor $BF_{>u}$ for the hypothesis $H_{>}$ and the unconstrained model for each study $t \in T$.

The third row consists of the Bayes Factor $BF_{<u}$ for the hypothesis $H_{<}$ and the unconstrained model for each study $t \in T$.

**Example Output**

```{r exbfmu, echo = FALSE}
ex<- unconstrained.BF(ex)
ex$BFmu
```


##Calculating Posterior Model Probabilities (PMP) from Prior Model Probabilities

This section will consist of two functions: **pi0.to.1** and **pi.0m.t.func** 

**pi0.to.1**

This function calculates a prior model probability ($\pi^{1}_{m}$)for a hypothesis $H_{m}$. Where $\pi^{1}_{m}$ gives the relative support for $H_{m}$ in a finite set of hypotheses.

The function does the following: 

$$
\pi^{1}_{m} = \frac{\pi^{0}_{m} BF_{mu}}{\pi^{0}_{0} BF_{0u} + \pi^{0}_{>} BF_{>u} + \pi^{0}_{<} BF_{<u}}
$$


```{r expito1}
### Posterior Model Probabilities for Hm  = pi1
# single update 
pi0.to.1<- function(pi0,BF){
  # Pi0 are the prior Model Probabilities and BF are the Bayes factors BF.mu for a single study
  # Careful not to use the whole matrix of BF's here !
  
  out<- (pi0*BF) / sum(pi0*BF) 
  out
}
```

**Summary of input / output**

__Inputs__

- pi0: Pi0 are the prior Model Probabilities.

- BF:Bayes factors for a single study across all hypotheses.

__Outputs__

The output for this function is a vector of length 3, which is the PMP for the given inputs. 

**Example Output**

```{r expito1cd,echo = FALSE}
pi0.to.1(ex$pi0,ex$BF[,1])
```

**update.PMP(BFcombo)**

This function does the updating evidence step of the method. It implements the following: 

$$
\pi^{0}_{t,m} = \pi^{1}_{t -1,m} for t = 2, ... ,T
$$

And then uses the **pi0.to.1** to update and calculate the PMP values for the set of studies.

```{r exupdate}
## Updating evidence : 
# step 1 find pi.0m.t where t is the study number

update.PMP<- function(BFcombo){
  if(any(names(BFcombo) == "BFmu")){
    pmp.t<- BFcombo$BFmu
    bfcols<- ncol(BFcombo$BFmu)
    pmp.t[,1]<- pi0.to.1(BFcombo$pi0,BFcombo$BFmu[,1])

    for(i in 2:bfcols){
      pmp.t[,i]<- pi0.to.1(pmp.t[,i-1],BFcombo$BFmu[,i])
    }
    names.vec<- rep(0, bfcols)
    for(i in 1:bfcols){
      names.vec[i]<- paste0("PMP", i )
    }
    colnames(pmp.t)<- names.vec
    BFcombo$PMP<- pmp.t
    return(BFcombo)
  } else{
    if(class(BFcombo) == "BFcombo"){
      stop("no BFmu section in the input, please run unconstrained.BF function")

    }else{
      stop("Object not of BFcombo class, please see make.BFcombo function")
    }
  }

}
```
## Summary of inputs and outputs

__Inputs__

- BFcombo is an object of class 'BFcombo' 

__Outputs__

This outputs an object of class BFcombo with a section which contains a matrix of PMP values. This is the final step in the pipeline.

**Example Output**

```{r expmpfin , echo = FALSE}
ex<- update.PMP(ex)
ex
```


##Wrapper Function: __calculate.PMP__

The following function is a wrapper for all of the above. It implements the method as seen in Kuiper et al.

**Summary of input / output**

__Inputs__

- beta1: The predicted means of the data. 
- var1 : The predicted variances of the data.
- pi0: The prior model probablilities, default will be to give each hypothesis equal probability so $\pi_{m}^{0} = \frac{1}{3} , m \in \{0 ,>,< \}$ 
- beta0: This is the prior mean, which has been set to 0 throughout. 
- var.mult = 1 : A multiplier of the prior variance, used in Kuiper et al to test the method. Default set at 1.

```{r PMP}

calculate.PMP <- function(beta1, var1,pi0,beta0, n = 10000, var.mult = 1){

  BFcombo <- prior.var(beta1 = beta1, var1 = var1, beta0 = beta0, pi0 = pi0)
  BFcombo$var0 <- BFcombo$var0 * var.mult
  BFcombo <- u.post.param(BFcombo)
  BFcombo <- unconstrained.BF(BFcombo,n)
  PMP <- update.PMP(BFcombo)
  PMP
}

```

# Testing the code : 

I aim to reproduce the results in the Kuiper et al paper. They have used the following to produce a series of results: 

##Table 1. Dummy data from Kuiper et al 

Study | $\sigma$ | $\beta$ No effect | Small Effect | Mixed Effect 1 | Mixed Effect 2 | Mixed Effect 3 
---|---|---|---|---|---|---
1 | 0.029 | 0.007 | 0.045 | 0.055 | 0.068 | 0.009
2 | 0.054 | 0.014 | 0.084 | -0.102 | -0.084 | -0.084
3 | 0.093 | 0.078 | 0.175 | 0.153 | 0.175 | 0.175
4 | 0.179 | 0.151 | 0.337 | 0.151 | 0.337 | 0.337 



------

Using these values I will run the function for the above and compare them to the results in the Kuiper et al paper.

First I will ensure the prior variances are the same as Kuiper et al:

##Table 2. Prior variances for dummy data in Kuiper et al.

```{r varcheck, echo = FALSE}
beta0<- 0
pi0<- rep(1/3 ,3)
var1<- c(0.029,0.054,0.093,0.179)
var1<- var1^2
#No effect
beta1<- c(0.007,0.014,0.078,0.151)
NEvar0<- prior.var(beta1 = beta1, var1 = var1, beta0 = 0, pi0 = pi0 )
NEvar0<-NEvar0$var0
#Small effect
beta1<- c(0.045,-0.084,0.175,0.337)
SEvar0<- prior.var(beta1 = beta1, var1 = var1, beta0 = 0, pi0 = pi0 )
SEvar0<-SEvar0$var0
#Mixed Effect 1 
beta1<- c(0.055,-0.102,0.153,0.151)
ME1var0<- prior.var(beta1 = beta1, var1 = var1, beta0 = 0, pi0 = pi0 )
ME1var0<-ME1var0$var0
### mixed effect 2 
beta1<- c(0.068,-0.084,0.175,0.337)
ME2var0<- prior.var(beta1 = beta1, var1 = var1, beta0 = 0, pi0 = pi0 )
ME2var0<-ME2var0$var0
### mixed effect 3 
beta1<- c(0.009,-0.084,0.175,0.337)
ME3var0<- prior.var(beta1 = beta1, var1 = var1, beta0 = 0, pi0 = pi0 )
ME3var0<-ME3var0$var0
```


Study $\sigma^{2}_{0}$ |  No effect | Small Effect | Mixed Effect 1 | Mixed Effect 2 | Mixed Effect 3 
---|---|---|---|---|---
1 | `r NEvar0[1]` | `r SEvar0[1]` | `r ME1var0[1]` | `r ME2var0[1]` | `r ME3var0[1]` 
2 | `r NEvar0[2]` | `r SEvar0[2]` | `r ME1var0[2]` | `r ME2var0[2]` | `r ME3var0[2]` 
3 | `r NEvar0[3]` | `r SEvar0[3]` | `r ME1var0[3]` | `r ME2var0[3]` | `r ME3var0[3]`  
4 | `r NEvar0[4]` | `r SEvar0[4]` | `r ME1var0[4]` | `r ME2var0[4]` | `r ME3var0[4]`



```{r testdatasetup, echo = FALSE}
##Sigma 
# No effect page 76

beta1<- c(0.007,0.014,0.078,0.151)

NEhalf<- calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 0.5)$PMP[,4]
NEone<- calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult =1)$PMP[,4]
NEtwo<- calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult =2)$PMP[,4]

# small effect

beta1<- c(0.045,0.084,0.175,0.337)
SEhalf<- calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 0.5)$PMP[,4]
SEone<- calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 1)$PMP[,4]
SEtwo<- calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 2)$PMP[,4]

# Mixed effect page 76
beta1<- c(0.055,-0.102,0.153,0.151)
ME1half<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 0.5)$PMP[,4]
ME1one<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 1)$PMP[,4]
ME1two<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 2)$PMP[,4]

### mixed effect 2 
beta1<- c(0.068,-0.084,0.175,0.337)
ME2half<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 0.5)$PMP[,4]
ME2one<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 1)$PMP[,4]
ME2two<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 2)$PMP[,4]

### mixed effect 3 
beta1<- c(0.009,-0.084,0.175,0.337)
ME3half<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 0.5)$PMP[,4]
ME3one<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 1)$PMP[,4]
ME3two<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 2)$PMP[,4]



```

##Table 3. Table of results for dummy data in Kuiper et al. 

$H_{m}$ | $\sigma$ multiplier | No effect | Small Effect | Mixed Effect 1 | Mixed Effect 2 | Mixed Effect 3
---|---|---|---|---|---|---
$H_{0}$ | 0.5 | `r NEhalf[1]` | `r SEhalf[1]` | `r ME1half[1]` | `r ME2half[1]` | `r ME3half[1]`  
$H_{>}$ | 0.5 | `r NEhalf[2]` | `r SEhalf[2]` | `r ME1half[2]` | `r ME2half[2]` | `r ME3half[2]`  
$H_{<}$ | 0.5 | `r NEhalf[3]` | `r SEhalf[3]` | `r ME1half[3]` | `r ME2half[3]` | `r ME3half[3]` 
---|---|---|---|---|---|---
$H_{0}$ | 1 | `r NEone[1]` | `r SEone[1]` | `r ME1one[1]` | `r ME2one[1]` | `r ME3one[1]`  
$H_{>}$ | 1 | `r NEone[2]` | `r SEone[2]` | `r ME1one[2]` | `r ME2one[2]` | `r ME3one[2]`  
$H_{<}$ | 1 | `r NEone[3]` | `r SEone[3]` | `r ME1one[3]` | `r ME2one[3]` | `r ME3one[3]` 
---|---|---|---|---|---|---
$H_{0}$ | 2 | `r NEtwo[1]` | `r SEtwo[1]` | `r ME1two[1]` | `r ME2two[1]` | `r ME3two[1]`  
$H_{>}$ | 2 | `r NEtwo[2]` | `r SEtwo[2]` | `r ME1two[2]` | `r ME2two[2]` | `r ME3two[2]`  
$H_{<}$ | 2 | `r NEtwo[3]` | `r SEtwo[3]` | `r ME1two[3]` | `r ME2two[3]` | `r ME3two[3]` 

    
All the results in both Table 2 and Table 3 are the same as Kuiper et al. 

## Results

Kuiper et al summarise the results in Table 3 by seeing which hypothesis receives the greatest support and if this result is 'sensitive'. They deem a result to suffer from sensitivity if one of the hypotheses considered which recieves the greatest support, doesnt recieve the greates suppost when it the same analysis is run on half of the variance and twice of the variace. 

Using the same criteria, the results in table 3 can be summarised as follows: 

### Table 4. Summary of results

   |  No effect | Small Effect | Mixed Effect 1 | Mixed Effect 2 | Mixed Effect 3
---|---|---|---|---
Conclusions | More studies needed | Support $H_{>}$ | More studies needed | Support $H_{>}$ | Support $H_{>}$
Reasons | Sensitivity and weak support | Not enough support | Strong support | weaker support but accept

# Bayes Safety Factor

## Concept and Usage

When using the BayesCombo package, the default is to set the prior model probabilities to be equal, thus giving each hypothesis the same prior probability, however the user of the package may be sceptical that this is the case. The Bayes Safety factor aims to help quanity just how sceptical one could be and still obtain the same final outcome in terms of choosing a hypothesis.

A hypothesis is picked and is given incrementally more weight, the method is then run using these prior model probabilities and the finial posterior model probability is recorded. The first set of prior model probabilities such that the final posterior model probabily of the hypothesis of which one is sceptical is equal to 0.95 after rounding, is called the boundary. This is becasue it represents the point at which one would potentially change the conclusion of the results, and if this occurs with a small change in prior model probabilities, one may reconsider their conclusion depending on the context of the problem. 

## Calculating the Bayes Safety factor: __BSfactor__

This function impletents the Bayes Safety factor:

```{r BFactor}
BSfactor <- function(beta1, var1,threshold = 0.95, beta0 = 0, n = 100, hypothesis = 1, reverse = FALSE){
  
  values <- seq(0.33333, 0.99999,length.out = n)
  
  if (reverse == FALSE){
    priors <- cbind( "H=0"=values,"H:>"=(1-values)/2, "H:<"=(1-values)/2)
    tot.len <- length(priors[,1])
    studies <- length(beta1)
    output <-priors
    output[]<- 0
    
    for(i in 1:tot.len){
      
      temp <- calculate.PMP(beta1, var1, beta0, pi0 = priors[i,])
      output[i,] <- temp$PMP[,studies]
      
    }
    
    
    if(hypothesis == 1 & output[1,2] < threshold){
      warning("The probability for the hypothesis H:> is not greater than threshold at the first step")
    }
    
    if(hypothesis == 2 & output[1,3] < threshold){
      warning("The probability for the hypothesis H:< is not greater than threshold at the first step")
    } else if(sum(round(output[,1+hypothesis], digits = 2) == threshold) > 0){
            boundary<- priors[round(output[,1+hypothesis], digits = 2) == threshold,]
      if(sum(round(output[,1+hypothesis], digits = 2) == threshold) > 1 ){
        boundary<- boundary[1,]
      }

    } else{
      warning("Not rounding to threshold , boundary = NULL")
      boundary<- NULL
    }
    return( structure(list(PMP = output, priorMP = priors, boundary = boundary, hypothesis = hypothesis ),class = "BSfactor"))
  }else{
   
    if(hypothesis == 1){
      
      priors <- cbind( "H=0"= (1-values)/2, "H:>"= values, "H:<"=(1-values)/2)
    }
    
    if(hypothesis == 2){
      
      priors <- cbind( "H=0"= (1-values)/2, "H:>"= (1-values)/2, "H:<"= values)
    }
    
    tot.len<- length(priors[,1])
    studies<- length(beta1)
    output<-priors
    output[]<- 0  
    
    for(i in 1:tot.len){
      
      temp<- calculate.PMP(beta1, var1, beta0, pi0 = priors[i,])
      output[i,]<-temp$PMP[,studies]
      
    }
    
    
    
    
    if(sum(round(output[,1+hypothesis], digits = 2) == threshold) > 0){
      boundary<- priors[round(output[,1+hypothesis], digits = 2) == threshold,]
      if(sum(round(output[,1+hypothesis], digits = 2) == threshold) > 1 ){
        boundary<- boundary[1,]
      }
    } else{
      warning("Not rounding to threshold , boundary = NULL")
      boundary<- NULL
    }
    return( structure(list(PMP = output, priorMP = priors, boundary = boundary, hypothesis = hypothesis ),class = "BSfactor"))
  }
}

```

### Summary of input and output

__Inputs__

- beta1	is a vector of observed means.

- var1 is a vector of observed variances.

- threshold is the value where the boundary will be assigned.

- beta0	is a vector or value of prior means. This is usually 0. 

- n is the number of steps between 0.33333 and 0.99999 for the hypothesis in question.

- hypothesis is a parameter which changes depending on context. See reverse parameter for more. 

- reverse : When false, this will mean that the prior model probabilities for the null hypothesis will increase and the value of hypothesis will determine which whypothesis will need to reach the boundary. In the case where reverse is true the value of hypothesis will determine which hypothesis will have increasing prior model probability. 


__Outputs__

The output consists of a BSfactor object with several sections:

 - PMP gives the final PMP values for the pipeline with the prior model probabilities given as the same row in the priorMP section. 

- priorMP gives the prior model probabilities used in the pipeline. 

- boundary will give NULL if there is no boundary, or the first set of prior model probabilities for which, when rounded, give the value of the threshold. 

- hypothesis gives the value of the hypothesis used. See inputs for more information. 

**Example Output**

```{r BSexample}

x <- BSfactor( beta1 = c(0.068,-0.084,0.175,0.337), var1 = c(0.000841,0.002916,0.008649,0.032041), beta0 = 0 ,n = 12)

x

```
#Visualisations

This section does not compute anything else, it only visualises certain aspects of the pipeline. The plots require the package *ggplot2*. 

```{r loadgg,message=FALSE,warning=FALSE}
library(ggplot2)
```

## Plotting prior and observed confidence interavals __plot.BFcombo__

This function plot all of the confidence and credibility intervals for a given percentage and a set of observed means and standard deviations.


```{r confint}
plot.BFcombo<-function(BFcombo,percent = 99){
  #Setup
  n.studies<- length(BFcombo$beta1)
  data<- matrix(0, ncol = 2 , nrow = n.studies)
  y<- seq(0,1,length.out = 2*n.studies)
  y1<-y[seq_along(y)%% 2 == 0]
  y2<-y[seq_along(y)%% 2 != 0]

  #begin calculation
  percent<- (percent / 100)
  percent<- percent + (1-percent)/2
  multiplier<- qnorm(percent)
  # get min/max
  data[,1]<- BFcombo$beta1 - sqrt(BFcombo$var1)*multiplier
  data[,2]<- BFcombo$beta1 + sqrt(BFcombo$var1)*multiplier
  bounds<- abs(data)
  output <- apply(X = bounds,MARGIN = 1,max)

  X3<- 0 - output
  X4 <-0 + output

  data<- data.frame(data,X3,X4, y1,y2,BFcombo$beta1)

  ## ggplot

  p<- ggplot(data, aes(X1,y1))
  p<- p + geom_point(aes(X1,y1), shape = 124,size = 4) + geom_point(aes(X2,y1),shape = 124,size = 4)
  p<- p + geom_vline(xintercept = 0,linetype = 2 )
  p<- p + geom_segment(aes(x =X1, y = y1 , xend = X2, yend = y1))
  p<- p + geom_point(aes(X3,y2),col = "red" ,shape = 124,size = 4) + geom_point(aes(X4,y2), col = "red",shape = 124,size = 4)
  p<- p + geom_segment(aes(x =X3, y = y2 , xend = X4, yend = y2),col = "red")
  p<- p + geom_point(aes(BFcombo$beta1,y1))
  p<- p + geom_text(aes(BFcombo$beta1,y1,label = paste("beta" , "^",1,1:n.studies,"*", "'' %+-% '' ","*", round(multiplier,3),"*","sigma","^",11:14, sep = "")),parse = TRUE,nudge_y = -0.03)
  p<- p + geom_text(aes(rep(0,4),y2,label = paste("beta" , "^",1:n.studies,"*", "'' %+-% '' ","*", round(multiplier,3),"*","sigma","^",1:4, sep = "")),parse = TRUE,nudge_y = -0.03,col = "red")
  p<- p + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
  p<- p + xlab(NULL) + ylab(NULL)
  p<- p + ggtitle("Prior Variance Selection")
  p
}
```

An example of the output of the *plot.BFcombo* function is: 

```{r exmplplot1, fig.height= 4 , fig.width=4}
beta1<- c(0.068,-0.084,0.175,0.337)
sig1<- c(0.029,0.054,0.093,0.179)
plot(calculate.PMP(beta1 = beta1,var1 = (sig1)^2, pi0 = pi0, beta0 = 0 ))
```

## Plotting posterior model probabilities:

**PMP.plot(data,pi0 = rep(1/3,3))**

To visualize the PMPs given by the pipeline I plot the lines given by the three hypotheses. This aims to show how the PMPs change depending on which studies are considered. 

```{r plotres}
PMP.plot<- function(BFcombo,pi0 = rep(1/3,3)){
  n.studies <- ncol(BFcombo$PMP)
  BFcombo$PMP<- cbind(pi0,BFcombo$PMP)
  plot(x = 0:n.studies,y = BFcombo$PMP[1,], type = "l", ylim = c(0,1),col = "red", ylab = "Probability", xlab = "PMP",
       main = "Plot of PMP probabilities and initial PrMP")
  lines(0:n.studies,BFcombo$PMP[2,], col = "blue")
  lines(0:n.studies,BFcombo$PMP[3,], col = "green")
  legend("topleft", legend = c("H:0","H:>","H:<"), fill = c("red","blue","green"))
}


```

```{r test, fig.height=4 , fig.width=6}
# Mixed effect 1 parameters
beta1<- c(0.055,-0.102,0.153,0.151)
var1<- c(0.029,0.054,0.093,0.179)
var1<- var1^2

ME1oneplot<-calculate.PMP(beta1,var1,pi0,beta0 = 0,var.mult = 1)
PMP.plot(ME1oneplot)
```
